[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "My name is Shohjahon. I got my BS degree in Computer Science and Engineering in 2020 from Inha University in Tashkent. This blog is my way to learn new things and teach others what I have learnt. By writing about machine learning and deep learning, I am also willing to deepen my knowledge on it. This blog is all about practical side of machine learning. I will share end-to-end pipeline of making ML models. I may write about theoretical part in future.\nMainly, I will share my knowledge on building models, deploying models and some useful stuff about Kaggle competitions in which I participate or find interesting. Each material will be clearly explained, for sure. My primary interest is Computer Vision and Natural Language Processing. Most posts will be on these topics, but I will write about other areas of Machine Learning too."
  },
  {
    "objectID": "posts/2022-04-24-vision-transformer.html",
    "href": "posts/2022-04-24-vision-transformer.html",
    "title": "Vision Transformer - All you need to know.",
    "section": "",
    "text": "It is a fact that Convolutional Neural Networks(CNN) have been dominant in Computer Vision tasks. However, ViT - AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE paper showed great results compared to SotA models. Here, we will dive deep enough to understand the Transformers architecure and in the next blogpost apply the model to some practical tasks. First, let’s start from the beginning - Encoder-Decoder Framework."
  },
  {
    "objectID": "posts/2022-04-24-vision-transformer.html#patch-embedings",
    "href": "posts/2022-04-24-vision-transformer.html#patch-embedings",
    "title": "Vision Transformer - All you need to know.",
    "section": "Patch Embedings",
    "text": "Patch Embedings\nNow, we will take a closer look at each one of the steps with code.\n\n\n\n\n\n\nNote\n\n\n\nAll PyTorch code is taken from here\n\n\n\n\nCode\nimport torch\nimport torch.nn as nn\n\n\n\n\nCode\nclass PatchEmbed(nn.Module):\n    \"\"\"\n    Split image into patches and embed them.\n    \n    Parameters\n    ----------\n    img_size: Size of the image\n    patch_size: Size of the path\n    in_chans: Number of input channels\n    embed_dim: The embedding dimension\n        \n    Attributes\n    ----------\n    n_patches: int\n        Number of patches inside of our image.\n    proj: nn.Conv2D\n        Convolutional layer that does both the splitting into patches and teir embedding.\n\n    \"\"\"\n    def __init__(self, img_size: int, path_size: int, in_chans: int = 3, embed_dim: int = 768):\n        super().__init__()\n        self.img_size = img_size\n        self.patch_size = patch_size\n        self.n_patches = (img_size // patch_size) ** 2\n        \n        self.proj = nn.Conv2d(\n            in_chans,\n            embed_dim,\n            kernel_size=patch_size,\n            stride=patch_size,\n        )\n    \n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\" Forward pass\n        Parameters\n        ----------\n        x: Shape\n        \"\"\"\n        x = self.proj(\n            x\n        )\n        x = x.flatten(2)\n        x = x.transpose(1, 2)\n        return x\n    \n\n\nWe will start with the 1st and 2nd step combined. Insted of dividing an input image into patches and then doing Linear Projection step, we combine them with the help of Convolution. The module accepts image size, patch size (originally 16x16), channels (1 - if it is graysclae, 3 - if it is RGB) and embedding dimension (stays constant across the entire network). Pay attention to Convolution operation - we set the kernel_size and stride equal to path_size. This way, the kernel exactly fall into the patches without overlapping. This way we get Patch Embeddings."
  },
  {
    "objectID": "posts/2022-04-24-vision-transformer.html#attention-1",
    "href": "posts/2022-04-24-vision-transformer.html#attention-1",
    "title": "Vision Transformer - All you need to know.",
    "section": "Attention",
    "text": "Attention\n\n\nCode\nclass Attention(nn.Module):\n    \"\"\"Attention mechanism.\n    \n    Parameters\n    ----------\n    dim: The input and output dimension of per token features.\n    \n    n_heads: Number of attention heads\n    \n    qkv_bias: If True then we include bias to the query, key and value projections.\n    \n    attn_p: Dropout probability applied to the query, key and value tensors.\n    \n    proj_p: Dropout probability applied to the output tensor\n    \n    Attributes\n    ----------\n    scale: Normalizing for the dot product\n    \n    qkv: Linear projection for the query, key and value.\n    \n    proj: Linear mapping that takes in the concatenated output of all attention heads and maps it into a new space.\n    \n    attn_drop, proj_drop: Droput layers.\n    \"\"\"\n    \n    def __init__(self, dim: int, n_heads: int = 12, qkv_bias: bool = True, attn_p: float = 0.2, proj_p: float = 0.2):\n        super().__init__()\n        self.n_heads = n_heads\n        self.dim = dim\n        self.head_dim = dim // n_heads\n        self.scale = self.head_dim ** -0.5\n        \n        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n        self.attn_drop = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(proj_p)\n        \n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Run forward pass.\n        \n        Parameters\n        ----------\n        x : Shape (n_samples, n_patches + 1, dim).\n        \"\"\"\n        n_samples, n_tokens, dim = x.shape\n        \n        qkv = self.qkv(x)\n        qkv = qkv.reshape(\n            n_samples, n_tokens, 3, self.n_heads, self.head_dim\n        )\n        qkv = qkv.permute(\n            2, 0, 3, 1, 4\n        )\n            \n        q, k, v = qkv[0], qkv[1], qkv[2]\n        k_t = k.transpose(-2, -1)\n        dp = (\n           q @ k_t\n        ) * self.scale \n        attn = dp.softmax(dim=-1) \n        attn = self.attn_drop(attn)\n\n        weighted_avg = attn @ v  \n        weighted_avg = weighted_avg.transpose(\n                1, 2\n        )\n        weighted_avg = weighted_avg.flatten(2) \n\n        x = self.proj(weighted_avg) \n        x = self.proj_drop(x)  \n\n        return x\n\n\nThe above code does 4 things: 1. Project each patch embedding into three vectors called query, key and value. 2. Compute attention scores using dot-product. Queries and keys that are similar will have a large dot product. 3. Compute attention weights. Dot products can generate large numbers, which can destabilize the training process. To handle this, the attention scores are first multiplied by a scaling vector and then normalized with a softmax to ensure all the column values add up to 1. 4. Update the embeddings by multiplying them with the value vector."
  },
  {
    "objectID": "posts/2022-04-24-vision-transformer.html#mlp-layer",
    "href": "posts/2022-04-24-vision-transformer.html#mlp-layer",
    "title": "Vision Transformer - All you need to know.",
    "section": "MLP Layer",
    "text": "MLP Layer\n\n\nCode\nclass MLP(nn.Module):\n    \"\"\"Multilayer perceptron.\n    \n    Parameters\n    ----------\n    in_features : Number of input features.\n    hidden_features : Number of nodes in the hidden layer.\n    out_features : Number of output features.\n    p : Dropout probability.\n    Attributes\n    ----------\n    fc : nn.Linear\n        The First linear layer.\n    act : nn.GELU\n        GELU activation function.\n    fc2 : nn.Linear\n        The second linear layer.\n    drop : nn.Dropout\n        Dropout layer.\n    \"\"\"\n    def __init__(self, in_features: int, hidden_features: int, out_features: int, p: float = 0.2):\n        super().__init__()\n        self.fc1 = nn.Linear(in_features, hidden_features)\n        self.act = nn.GELU()\n        self.fc2 = nn.Linear(hidden_features, out_features)\n        self.drop = nn.Dropout(p)\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Run forward pass.\n        Parameters\n        ----------\n        x : Shape\n        Returns\n        -------\n        Shape\n        \"\"\"\n        x = self.fc1(\n                x\n        ) \n        x = self.act(x) \n        x = self.drop(x)  \n        x = self.fc2(x)  \n        x = self.drop(x)  \n\n        return x\n\n\nThere is nothing extraordinary in the above code except GELU activation function. This is classification part consisting of just two fully connected layers.\n\n\nCode\nclass Block(nn.Module):\n    \"\"\"Transformer block.\n    Parameters\n    ----------\n    dim : Embeddinig dimension.\n    n_heads : Number of attention heads.\n    mlp_ratio : Determines the hidden dimension size of the `MLP` module with respect\n        to `dim`.\n    qkv_bias : If True then we include bias to the query, key and value projections.\n    p, attn_p : Dropout probability.\n    Attributes\n    ----------\n    norm1, norm2 : LayerNorm\n        Layer normalization.\n    attn : Attention\n        Attention module.\n    mlp : MLP\n        MLP module.\n    \"\"\"\n    def __init__(self, dim: int, n_heads: int, mlp_ratio:float = 4.0, qkv_bias: bool = True, p: float = 0., attn_p: float = 0.):\n        super().__init__()\n        self.norm1 = nn.LayerNorm(dim, eps=1e-6)\n        self.attn = Attention(\n                dim,\n                n_heads=n_heads,\n                qkv_bias=qkv_bias,\n                attn_p=attn_p,\n                proj_p=p\n        )\n        self.norm2 = nn.LayerNorm(dim, eps=1e-6)\n        hidden_features = int(dim * mlp_ratio)\n        self.mlp = MLP(\n                in_features=dim,\n                hidden_features=hidden_features,\n                out_features=dim,\n        )\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Run forward pass.\n        Parameters\n        ----------\n        x : Shape.\n        \n        \"\"\"\n        x = x + self.attn(self.norm1(x))\n        x = x + self.mlp(self.norm2(x))\n\n        return x\n\n\nIn ViT authors used only one block (Encoder block). Original paper uses post layer normalization arrangement when normalization layer is placed in between the skip connections. However, it is hard to train from scratch and therefore the code above uses pre layer normalization arrangement."
  },
  {
    "objectID": "posts/2022-04-24-vision-transformer.html#vision-transformer",
    "href": "posts/2022-04-24-vision-transformer.html#vision-transformer",
    "title": "Vision Transformer - All you need to know.",
    "section": "Vision Transformer",
    "text": "Vision Transformer\n\n\nCode\nclass VisionTransformer(nn.Module):\n    \"\"\"Simplified implementation of the Vision transformer.\n    Parameters\n    ----------\n    img_size : Both height and the width of the image (it is a square).\n    patch_size : Both height and the width of the patch (it is a square).\n    in_chans : Number of input channels.\n    n_classes : Number of classes.\n    embed_dim : Dimensionality of the token/patch embeddings.\n    depth : Number of blocks.\n    n_heads : Number of attention heads.\n    mlp_ratio : Determines the hidden dimension of the `MLP` module.\n    qkv_bias : If True then we include bias to the query, key and value projections.\n    p, attn_p : Dropout probability.\n    \n    Attributes\n    ----------\n    patch_embed : PatchEmbed\n        Instance of `PatchEmbed` layer.\n    cls_token : nn.Parameter\n        Learnable parameter that will represent the first token in the sequence.\n        It has `embed_dim` elements.\n    pos_emb : nn.Parameter\n        Positional embedding of the cls token + all the patches.\n        It has `(n_patches + 1) * embed_dim` elements.\n    pos_drop : nn.Dropout\n        Dropout layer.\n    blocks : nn.ModuleList\n        List of `Block` modules.\n    norm : nn.LayerNorm\n        Layer normalization.\n    \"\"\"\n    def __init__(\n            self,\n            img_size: int = 384,\n            patch_size: int = 16,\n            in_chans: int = 3,\n            n_classes: int = 1000,\n            embed_dim: int = 768,\n            depth: int = 12,\n            n_heads: int = 12,\n            mlp_ratio: int = 4.,\n            qkv_bias: int = True,\n            p: int = 0.2,\n            attn_p: int = 0.2,\n    ):\n        super().__init__()\n\n        self.patch_embed = PatchEmbed(\n                img_size=img_size,\n                patch_size=patch_size,\n                in_chans=in_chans,\n                embed_dim=embed_dim,\n        )\n        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n        self.pos_embed = nn.Parameter(\n                torch.zeros(1, 1 + self.patch_embed.n_patches, embed_dim)\n        )\n        self.pos_drop = nn.Dropout(p=p)\n\n        self.blocks = nn.ModuleList(\n            [\n                Block(\n                    dim=embed_dim,\n                    n_heads=n_heads,\n                    mlp_ratio=mlp_ratio,\n                    qkv_bias=qkv_bias,\n                    p=p,\n                    attn_p=attn_p,\n                )\n                for _ in range(depth)\n            ]\n        )\n\n        self.norm = nn.LayerNorm(embed_dim, eps=1e-6)\n        self.head = nn.Linear(embed_dim, n_classes)\n\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Run the forward pass.\n        Parameters\n        ----------\n        x : Shape `(n_samples, in_chans, img_size, img_size)`.\n        Returns\n        -------\n        logits : Logits over all the classes - `(n_samples, n_classes)`.\n        \"\"\"\n        n_samples = x.shape[0]\n        x = self.patch_embed(x)\n\n        cls_token = self.cls_token.expand(\n                n_samples, -1, -1\n        ) \n        x = torch.cat((cls_token, x), dim=1)  \n        x = x + self.pos_embed  \n        x = self.pos_drop(x)\n\n        for block in self.blocks:\n            x = block(x)\n\n        x = self.norm(x)\n\n        cls_token_final = x[:, 0]  # just the CLS token\n        x = self.head(cls_token_final)\n\n        return x\n\n\nFinally, it is time to assemble all the code blocks into one module to create Vision Transformer model. The code incorporates positional embedding and a [cls] token into the model and then add all other modules.\nIn this blog post we covered all about Transformer architecture, starting from encoder-decoder framework to application of Transformer architecture in Computer Vision. In the next post, we will apply the model in a classification task."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Central Limit",
    "section": "",
    "text": "(Almost) All You Need to Know about ROC and AUC.\n\n\n\n\n\n\n\nmetrics\n\n\nROC AUC\n\n\n\n\n\n\n\n\n\n\n\nSep 9, 2023\n\n\nShohjahon Nishonov\n\n\n\n\n\n\n  \n\n\n\n\nWeights and Biases - A minimum for logging and experiment tracking\n\n\n\n\n\n\n\nMLOps\n\n\nWeights&Biases\n\n\n\n\n\n\n\n\n\n\n\nJul 30, 2023\n\n\nShohjahon Nishonov\n\n\n\n\n\n\n  \n\n\n\n\nVision Transformer - All you need to know.\n\n\n\n\n\n\n\nDeep Learning\n\n\nComputer Vision\n\n\nTransformers\n\n\n\n\n\n\n\n\n\n\n\nApr 24, 2022\n\n\nShohjahon Nishonov\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2023-07-30-wb.html",
    "href": "posts/2023-07-30-wb.html",
    "title": "Weights and Biases - A minimum for logging and experiment tracking",
    "section": "",
    "text": "Experiment tracking is a vital step in a modern machine learning, as without it it would be difficult to compare results and choose best parameters for our models. Here, I will show the minimum to track experiments with the help of Weights&Biases(I will refer to it as WandB).\n\n\nWandB is a machine learning platform not only for keeping track of your hyperparameters, system metrics, and predictions so you can compare models, but also it enables to create ML workflows, version dataset and models, optimize hyperparameters and monitor models in production. It has several competitors. Namely, mlflow is one of the most famous ones. However, personally, I found it easier to set up and run WandB than mlflow. Moreover, it has a free tier plan, which is more than enough for personal needs.\nFirst of all, you need to register an account there, and after that go to the User Settings and under the API keys you will find your key to interact with the service.\nThen, you need to install WandB itself locally with:\n\npip install wandb\n\nNow, launch your favourite IDE (I would suggest jupyter-lab/jupyter notebook for prototyping) and connect to your dashboard with the following commands:\n\nimport wandb\n\nwandb.login['YOUR API KEY HERE']\n\nThere are 5 key commands in order to work with WandB:  1. wandb.login() - authorisation in the system 2. wandb.init() - new experiment initialization 3. wandb.log() - logging metrics 4. wandb.log_artifact() - logging artifacts 5. wandb.finish() - finishing experiment\nWe will look into all these commands with the help of XGBoost example.\nFirst, we’ll import libraries.\n\nfrom xgboost import XGBClassifier\nfrom xgboost.callback import EarlyStopping\nimport wandb\n\nfrom sklearn.model_selection import StratifiedKFold\nwandb.login()\n\nThen, we will write configuration for our model.\n\nConfig = dict(\n    n_splits = 5,\n    random_seed = 42,\n    #params for the model\n    objective = \"binary:logistic\",\n    tree_method = \"hist\",\n    n_estimators=200,\n    early_stopping=20,\n\n    # regularization\n    max_depth=5,\n    max_delta_step=17,\n    colsample_bytree=0.632831510106799,\n    colsample_bylevel=0.6390056763292044,\n    eta=0.487396497096089,\n    min_child_weight = 1,\n    gamma = 0.25490782392352085,\n    reg_lambda = 59.960195187994934,\n    reg_alpha = 8.529168659942826,\n    scale_factor=4.71\n)\n\nSome training code with cross-validation.\n\nclfs = []\nscores = []\nscores_eval = []\n#X = X_train.drop(cols2drop, axis=1)\n\nwandb.init(project=\"Project Name\",\n           config=Config,\n          group=\"xgboost\",\n          job_type=\"train\",\n          name = \"Training with parameters suggested with optuna with additional feats\")\n\n\nskf = StratifiedKFold(n_splits=Config[\"n_splits\"], shuffle=True, random_state=Config[\"random_seed\"])\n\nfor train_index, test_index in skf.split(X, y):\n    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n    \n    es = EarlyStopping(\n    rounds=Config[\"early_stopping\"],\n    min_delta=1e-3,\n    save_best=True,\n    maximize=True,\n    data_name=\"validation_0\",\n    metric_name=\"auc\",\n)\n    \n    clf = XGBClassifier(tree_method=Config[\"tree_method\"],\n            n_estimators=Config[\"n_estimators\"],\n            max_depth=Config[\"max_depth\"],\n            scale_pos_weight=Config[\"scale_factor\"],\n            max_delta_step=Config[\"max_delta_step\"],\n            colsample_bytree=Config[\"colsample_bytree\"],\n            colsample_bylevel=Config[\"colsample_bylevel\"],\n            learning_rate=Config[\"eta\"],\n            min_child_weight = Config[\"min_child_weight\"],\n            gamma = Config[\"gamma\"],\n            reg_lambda = Config[\"reg_lambda\"],\n            reg_alpha = Config[\"reg_alpha\"],\n            enable_categorical=True,\n            objective=Config[\"objective\"],\n            eval_metric=\"auc\",\n            random_seed=Config[\"random_seed\"],\n            callbacks=[es])\n            \n    \n            \n    \n    clfs.append(clf)\n    clf.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=10)\n    preds = clf.predict(X_valid) \n    auc_valid = roc_auc_score(y_valid, preds)\n    wandb.log({\"Valid AUC\": auc_valid})\n    scores_eval.append(auc_valid)\n    print(f\"Valid AUC {auc_valid}\")\n    \n    \n    wandb.log({'Train AUC': np.mean([v for k, v in clf.evals_result()[\"validation_0\"].items() if \"auc\" in k], dtype=\"float16\")})\n    scores.append(np.mean([v for k, v in clf.evals_result()[\"validation_0\"].items() if \"auc\" in k], dtype=\"float16\")) \n\n\nmean_score = np.mean(scores, dtype=\"float16\") - np.std(scores, dtype=\"float16\")\n\nprint(\"mean AUC score ---------&gt;\", mean_score)\nprint(f\"mean valid AUC score {np.mean(scores_eval, dtype='float16') - np.std(scores_eval, dtype='float16')}\")\n\nwandb.log({\"Mean AUC\": mean_score, \"Mean AUC valid\": np.mean(scores_eval)})\n\n\nclf.save_model(\"xgb_classificator.json\")\n\nartifact = wandb.Artifact(name='best_XGBoost', type='model')\nartifact.add_file('xgb_classificator.json')\nwandb.log_artifact(artifact)\n\nwandb.finish()\n\nWith the wandb.init() method we define our parameters for WandB. Namely, project parameter is for the project name config parameter is for the model config group is for your group of models job_type is whether your model is in training mode or inference name is for your experiment name\n\n\n\n\n\n\nTip\n\n\n\nI suggest giving meaningful name for your experimentation name, because it would be easier to find the right one among other experiments.\n\n\nWith wandb.log() you can track any metric. It should be in a python’s dictype data type.\nWandB artifacts is a way to save your input/output data and model.\n\n\n\n\n\n\nNote\n\n\n\n\nCreate an empty artifact with wandb.Artifact()\nAdd your model file or other files with wandb.add_file()\nCall wandb.log_artifact() to save your files.\n\n\n\nFinally, finish logging with wandb.finish().\nI hope, this small article help you to start using Weights and Biases. For more information about artifacts refer to this Colab notebook. And link for official documenation and examples."
  },
  {
    "objectID": "posts/2023-07-30-wb.html#so-what-is-wandb",
    "href": "posts/2023-07-30-wb.html#so-what-is-wandb",
    "title": "Weights and Biases - A minimum for logging and experiment tracking",
    "section": "",
    "text": "WandB is a machine learning platform not only for keeping track of your hyperparameters, system metrics, and predictions so you can compare models, but also it enables to create ML workflows, version dataset and models, optimize hyperparameters and monitor models in production. It has several competitors. Namely, mlflow is one of the most famous ones. However, personally, I found it easier to set up and run WandB than mlflow. Moreover, it has a free tier plan, which is more than enough for personal needs.\nFirst of all, you need to register an account there, and after that go to the User Settings and under the API keys you will find your key to interact with the service.\nThen, you need to install WandB itself locally with:\n\npip install wandb\n\nNow, launch your favourite IDE (I would suggest jupyter-lab/jupyter notebook for prototyping) and connect to your dashboard with the following commands:\n\nimport wandb\n\nwandb.login['YOUR API KEY HERE']\n\nThere are 5 key commands in order to work with WandB:  1. wandb.login() - authorisation in the system 2. wandb.init() - new experiment initialization 3. wandb.log() - logging metrics 4. wandb.log_artifact() - logging artifacts 5. wandb.finish() - finishing experiment\nWe will look into all these commands with the help of XGBoost example.\nFirst, we’ll import libraries.\n\nfrom xgboost import XGBClassifier\nfrom xgboost.callback import EarlyStopping\nimport wandb\n\nfrom sklearn.model_selection import StratifiedKFold\nwandb.login()\n\nThen, we will write configuration for our model.\n\nConfig = dict(\n    n_splits = 5,\n    random_seed = 42,\n    #params for the model\n    objective = \"binary:logistic\",\n    tree_method = \"hist\",\n    n_estimators=200,\n    early_stopping=20,\n\n    # regularization\n    max_depth=5,\n    max_delta_step=17,\n    colsample_bytree=0.632831510106799,\n    colsample_bylevel=0.6390056763292044,\n    eta=0.487396497096089,\n    min_child_weight = 1,\n    gamma = 0.25490782392352085,\n    reg_lambda = 59.960195187994934,\n    reg_alpha = 8.529168659942826,\n    scale_factor=4.71\n)\n\nSome training code with cross-validation.\n\nclfs = []\nscores = []\nscores_eval = []\n#X = X_train.drop(cols2drop, axis=1)\n\nwandb.init(project=\"Project Name\",\n           config=Config,\n          group=\"xgboost\",\n          job_type=\"train\",\n          name = \"Training with parameters suggested with optuna with additional feats\")\n\n\nskf = StratifiedKFold(n_splits=Config[\"n_splits\"], shuffle=True, random_state=Config[\"random_seed\"])\n\nfor train_index, test_index in skf.split(X, y):\n    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n    \n    es = EarlyStopping(\n    rounds=Config[\"early_stopping\"],\n    min_delta=1e-3,\n    save_best=True,\n    maximize=True,\n    data_name=\"validation_0\",\n    metric_name=\"auc\",\n)\n    \n    clf = XGBClassifier(tree_method=Config[\"tree_method\"],\n            n_estimators=Config[\"n_estimators\"],\n            max_depth=Config[\"max_depth\"],\n            scale_pos_weight=Config[\"scale_factor\"],\n            max_delta_step=Config[\"max_delta_step\"],\n            colsample_bytree=Config[\"colsample_bytree\"],\n            colsample_bylevel=Config[\"colsample_bylevel\"],\n            learning_rate=Config[\"eta\"],\n            min_child_weight = Config[\"min_child_weight\"],\n            gamma = Config[\"gamma\"],\n            reg_lambda = Config[\"reg_lambda\"],\n            reg_alpha = Config[\"reg_alpha\"],\n            enable_categorical=True,\n            objective=Config[\"objective\"],\n            eval_metric=\"auc\",\n            random_seed=Config[\"random_seed\"],\n            callbacks=[es])\n            \n    \n            \n    \n    clfs.append(clf)\n    clf.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=10)\n    preds = clf.predict(X_valid) \n    auc_valid = roc_auc_score(y_valid, preds)\n    wandb.log({\"Valid AUC\": auc_valid})\n    scores_eval.append(auc_valid)\n    print(f\"Valid AUC {auc_valid}\")\n    \n    \n    wandb.log({'Train AUC': np.mean([v for k, v in clf.evals_result()[\"validation_0\"].items() if \"auc\" in k], dtype=\"float16\")})\n    scores.append(np.mean([v for k, v in clf.evals_result()[\"validation_0\"].items() if \"auc\" in k], dtype=\"float16\")) \n\n\nmean_score = np.mean(scores, dtype=\"float16\") - np.std(scores, dtype=\"float16\")\n\nprint(\"mean AUC score ---------&gt;\", mean_score)\nprint(f\"mean valid AUC score {np.mean(scores_eval, dtype='float16') - np.std(scores_eval, dtype='float16')}\")\n\nwandb.log({\"Mean AUC\": mean_score, \"Mean AUC valid\": np.mean(scores_eval)})\n\n\nclf.save_model(\"xgb_classificator.json\")\n\nartifact = wandb.Artifact(name='best_XGBoost', type='model')\nartifact.add_file('xgb_classificator.json')\nwandb.log_artifact(artifact)\n\nwandb.finish()\n\nWith the wandb.init() method we define our parameters for WandB. Namely, project parameter is for the project name config parameter is for the model config group is for your group of models job_type is whether your model is in training mode or inference name is for your experiment name\n\n\n\n\n\n\nTip\n\n\n\nI suggest giving meaningful name for your experimentation name, because it would be easier to find the right one among other experiments.\n\n\nWith wandb.log() you can track any metric. It should be in a python’s dictype data type.\nWandB artifacts is a way to save your input/output data and model.\n\n\n\n\n\n\nNote\n\n\n\n\nCreate an empty artifact with wandb.Artifact()\nAdd your model file or other files with wandb.add_file()\nCall wandb.log_artifact() to save your files.\n\n\n\nFinally, finish logging with wandb.finish().\nI hope, this small article help you to start using Weights and Biases. For more information about artifacts refer to this Colab notebook. And link for official documenation and examples."
  },
  {
    "objectID": "posts/roc_auc/2023-09-09-roc.html",
    "href": "posts/roc_auc/2023-09-09-roc.html",
    "title": "(Almost) All You Need to Know about ROC and AUC.",
    "section": "",
    "text": "Background\nAUC ROC is one of the most widely used metrics for binary classification tasks in machine learning. However, I used it as a “black box” which quantifies the model result in a meaningful way without considering what is going on under the hood. I think, it is time to dive deeper into how AUC ROC is calculated and its relation with ROC curve.\nFirst things first, to construct ROC curve we need to understand confusion matrix, false positive rate, true positive rate and thresholds. Confusion matrix is a square matrix (2x2 in binary classification tasks) that summarizes model performance. The following image describes the confusion matrix: \nIf you are confused with the above imaga, here is an example from real life: \nAs for FPR and TPR, TPR is the number of positive classes correctly classified out of all positive classes and fpr is the number of negative classes incorrectly classified out of all negative classes.\n\\[False Positive Rate = \\frac{False Positive}{False Positive + True Negative}\\] \\[True Positive Rate = \\frac{True Positive}{True Positive + False Negative}\\]\nThreshold is a number that divides data into positive and negative classes based on its score or probability. For example, default threshold is 0.5 and if a model gives some data point a score of 0.51(in other words, probability of this exact point belonging to positive class is 0.51) it goes into positive class as its score is higher than the threshold and vice versa. These are all the things we needed to know before constructing ROC curves and now let’s build a roc curve from scratch.\n\n\nROC Curve Generation\nAlgorithm for building a ROC curve is as follows: 1. Set a threshold value between 0 and 1. 2. Make a prediction on the test set and get the probabilities. 3. Compute confusion matrix based on your threshold. 4. Compute fpr and tpr and plot on the graph. 5. Go to step 1.\nAs you can see the algorithm is pretty straightforward.\nHowever, there is even simpler way to construct roc curves. We will use monotonocity of the curve. In other words, if one point belongs to class 1 on some threshold, it won’t change on lower thresholds.\nFirst of all, we will import necessary libraries and make a dummy dataset and split it into train and test.\n\n\n\n\n\n\nNote\n\n\n\nNote that the code is simplified in order to explain the concept.\n\n\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_curve, roc_auc_score\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\nimport seaborn as sns\nimport pandas as pd\n\n\nX, y = make_classification(n_samples=2000, n_features=7, n_classes=2, random_state=42, weights=None)\n\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, shuffle=True, random_state=42)\n\n\n\nunique, cnts = np.unique(y_test, return_counts=True)\nprint(unique, cnts)\n\n[0 1] [393 407]\n\n\nNow, we fit simple logistic regression and acquire probabilities of a point belonging to class 1.\n\nclf = LogisticRegression(random_state=42).fit(X_train, y_train)\nprobs = clf.predict_proba(X_test)[:, 1]\n\nclf.classes_\n\narray([0, 1])\n\n\n\nscores = np.c_[y_test, probs]\n\n\ndf_scores = pd.DataFrame(scores)\nscores_sorted = df_scores.sort_values(by=1, axis=0, ascending=False)\nscores_sorted.columns = [\"Class\", \"Score\"]\nscores_sorted.head()\n\n\n  \n    \n\n\n\n\n\n\nClass\nScore\n\n\n\n\n592\n1.0\n0.999959\n\n\n270\n1.0\n0.999900\n\n\n487\n1.0\n0.999880\n\n\n146\n1.0\n0.999876\n\n\n503\n1.0\n0.999778\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\nNow, we have probabilities and we merge these probabilities with the true class labels, then sort the resulting dataframe like above.\nClassical algorithm of ROC curve construction is really simple. First, we define a data structure(in our case, list) to hold points and false_positive and true_positive variables to keep track of the values. Then, we traverse through the above dataframe, and use formulas to find FPR and TPR, and increment the correspoinding variable. The python code is as following:\n\nclass DATA:\n    R = []\n    FP, TP =0, 0\n    f_prev = -np.inf\n\ndef calc_roc(x):\n\n    if x.Score != DATA.f_prev:\n        DATA.R.append([DATA.FP/393, DATA.TP/407])\n        DATA.f_prev = x.Score\n    if int(x.Class) == 1:\n        DATA.TP += 1\n    else:\n        DATA.FP += 1\n\n\nscores_sorted.apply(calc_roc, axis=1)\nr_np = np.array(DATA.R)\n\nOne ambigious part of the algorithm is when we check current score value is not equal to the previous score value. Intuition behind this is, when we have equal score values with different class labels, we should take average values of both FPR and TPR in order to avoid optimistic or pessimistic evaluations. In our case, it is easier to drop such values. At the end, we plot the roc curve with the help of seaborn.\n\ng = sns.relplot(\n    x=r_np[:, 0],\n    y=r_np[:, 1],\n    kind=\"scatter\"\n)\ng.set_axis_labels(\"False Positive Rate\", \"True Positive Rate\")\n\n\n\n\nFortunately, scikit-learn module also can build ROC curves with the help of roc_curve function. We need to pass this function true values, probability scores and should tell which label is positive. In return, it gives us FPR, TPR and threshold values.\n\nfpr, tpr, thresholds = roc_curve(y_test, probs, pos_label=1, drop_intermediate=False)\n\n\ng = sns.relplot(\n    x=fpr,\n    y=tpr,\n    kind=\"scatter\"\n)\ng.set_axis_labels(\"False Positive Rate\", \"True Positive Rate\")\n\n\nAs you can see, two graphs are identical. drop_intermediate attribute is used to drop non-optimized threshold values in order to minimize number of points in our ROC curve.\n\nfpr, tpr, thresholds = roc_curve(y_test, probs, pos_label=1, drop_intermediate=True)\n\n\ng= sns.relplot(\n    x=fpr,\n    y=tpr,\n    kind=\"scatter\"\n)\ng.set_axis_labels(\"False Positive Rate\", \"True Positive Rate\")\n\n\n\n\n\n\nCharacteristics of a ROC curve\nROC curve, usually, is used to choose an optimal threshold for the task. Choosing an optimal threshold depends on the problem you are solving. For example, if your task is classifying patients based on the presence of cancer, it is important to you not to label people with cancer as healthy. Thus, you should minimize the number of false negatives, and you should choose the threshold with the maximum True Positive Rate. However, if you just want to choose an optimal threshold, there is a simple formula, Youden’s J satistic: \\[J = TruePositiveRate - FalsePositiveRate\\] You should choose \\(J\\) with maximum value.\nOne of the key features of ROC curve is that it is insensitive to the imbalance dataset. It will not change if you change the size of positive class or negative class, as TPR and FPR are independant.\n\n\nAUC (Area Under the Curve) ROC\nAUC ROC is just Area Under the ROC Curve. It gives us a number between 0 and 1. AUC ROC score of 1 means that your classifier is perfect, and 0 means that you are doing something wrong as random guessing will give us AUC score of 0.5. So, you do not want to use classifier which gives score under 0.5. Statistically, AUC represents the probability that a random positive variable will be ranked higher than a random negative variable.\nIn order to calculate the AUC you can use the following formula: \\[\\frac{(fpr_{i+1} - fpr_{i}) * (tpr_{i} + tpr_{i+1})}{2}\\]\nOr use sklearn’s roc_auc_score function.\nThis post only scratched the surface about ROC and AUC. If you want to go really deep, I would suggest the following article."
  }
]