{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "295ca9e5-da88-4149-9e14-887b7c9ffb28",
   "metadata": {},
   "source": [
    "# Vision Transformer - All you need to know.\n",
    "\n",
    "- toc: true\n",
    "- branch: master\n",
    "- comments: true\n",
    "- author: Shohjahon Nishonov\n",
    "- categories: [Deep Learning, Computer Vision, Transformers]\n",
    "- image: images/vit.jpeg\n",
    "- hide: false"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2acb7ce-2c4e-4555-a055-5fc9b22aceaf",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "It is a fact that Convolutional Neural Networks(CNN) have been dominant in Computer Vision tasks. However, [ViT - AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE](https://arxiv.org/abs/2010.11929 \"\") paper showed great results compared to SotA models. Here, we will dive deep enough to understand the Transformers architecure and in the next blogpost apply the model to some practical tasks.\n",
    "First, let's start from the beginning - Encoder-Decoder Framework."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608c3b01-bd60-4a7e-8f51-dc71d18f4a1f",
   "metadata": {},
   "source": [
    "# Encoder-Decoder Framework Basics\n",
    "Encoder-decoder framework is used for sequence-to-sequence tasks, for example, machine translation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479203ca-6af9-4849-80e7-e1b4c5ec2dd9",
   "metadata": {},
   "source": [
    "![alt text](../images/encoder-decoder.jpg \"Encoder-Decoder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54f3504-475d-4506-afc3-1a2e63c4b228",
   "metadata": {},
   "source": [
    "The simplest model consists of two RNNs: one for the encoder and another for the decoder. Encoder reads the source sentence and produces a context vector where all the information about the source sentence is encoded. Then, decoder reads the context vector and generates output sentence based on this vector. The problem with such a model is that encoder tries to compress the whole source sentence into a fixed size vector. This can be hard, especially with long text inputs. It cannot put all information into a single vector without loosing some meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703564aa-f6b4-49a9-99f1-4774701dbe91",
   "metadata": {},
   "source": [
    "> Note: Imagine explaining a topic with few words. Only hope is the other person understands it as you want."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf8a8a0-4a63-4293-80ee-ffa021159a99",
   "metadata": {},
   "source": [
    "# Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9e5622-18f3-46fc-a6f9-740f3367c7e1",
   "metadata": {},
   "source": [
    "Attention was introduced to overcome the shortcomings of the fixed vector representation problem. At each decoder step, it decides which source parts are more important. The encoder does not compress the whole input into a single vector - it gives context for all input tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d10591-bf3b-4015-b9aa-b8825a9f3143",
   "metadata": {},
   "source": [
    "![Attention](../images/general_scheme-min.png \"\")\n",
    "> Image from {% cite voita2020nlpCourse %}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0fde884-cf8c-4a51-84f8-4a33ed26f8d5",
   "metadata": {},
   "source": [
    "The mechanism is as follows:\n",
    "1. Attention mechanism takes all encoder states and one decoder state as input\n",
    "2. Gives a score to the input (computes weights)\n",
    "3. Output from 2 are normalized using softmax function.\n",
    "4. Outputs weighted sum."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b38382-fbe6-40e4-822c-b34633450227",
   "metadata": {},
   "source": [
    "![](../images/attention.jpg \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716e0d73-c8f8-4e87-9974-f7fdfafc3c75",
   "metadata": {},
   "source": [
    "But, how the score is calculated?\n",
    "The simplest method is dot product of encoder states and a decoder state. Other methods are bilinear function and multi-layer perceptron which was proposed in the original paper. The main purpose of calculating the score is identifying similarity between the current input and all other inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9030cc99-fa29-45a4-bc57-d8d63a5aa4c5",
   "metadata": {},
   "source": [
    "# Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a0512d-6c71-483d-a036-9eb30b9fb6c2",
   "metadata": {},
   "source": [
    "Transformer model was introduced in the paper [Attention is All You Need](https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf \"\") in 2017. It uses only attention mechanisms: without RNN or CNN. It has become a go to model for not only sequence-to-sequence tasks but also for other tasks.\n",
    "Let me show you a demonstration of Transformer from [Google AI blog post](https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html \"\")."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54b2676-3cda-45c9-bb31-75fe1c24cc05",
   "metadata": {},
   "source": [
    "![](../images/transformer_original.gif \"Transformer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6ac775-bd51-447b-897a-e70b29bf7b54",
   "metadata": {},
   "source": [
    "Let's take the sentence \"The bank of the river.\" as an example. The word \"bank\" might confuse RNN as it processes the sentence sequentially and does not know whether the \"bank\" represents financial institution or the edge of the river untill the model reaches the end of the sentence. Unlike RNN, Transformer knows the context without reading the whole sentence as it's encoder tokens interact each other and give context to the words. In the above example, it is clear that at each step tokens exchange information and try to understand each other better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93a01e6-a962-4280-ba4b-137f6e4dd26d",
   "metadata": {},
   "source": [
    "The main part of the model is self-attention mechanism. The difference between self-attention and simplified attention is the prior has trainable weights.\n",
    "\n",
    "$$query = W^qx_i\\newline\n",
    "key = W^kx_i\\newline\n",
    "value = W^vx_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3577e4-bcf4-4b8d-86b4-bbb06ad11a8d",
   "metadata": {},
   "source": [
    "![](../images/self-att.png \"Self Attention\")\n",
    "> Image from [here](https://sebastianraschka.com/pdf/lecture-notes/stat453ss21/L19_seq2seq_rnn-transformers__slides.pdf \"Sebastian Raschka, Introduction to Deep Learning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1666ae-74c0-4cab-974f-adc969d07f0e",
   "metadata": {},
   "source": [
    "As shown in the example above, we calculate query, key and value for every input token. Output of self-attention is calculated like simplified attention with slight differences:\n",
    "\n",
    "$$Attention(q,k,v) = softmax(score)v\\newline$$\n",
    "Here\n",
    "$$score = \\frac{qk^T}{\\sqrt{d_k}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436461c2-b0a3-4857-8fbd-85db0ae80fa1",
   "metadata": {},
   "source": [
    "The reason why we are using scaled dot-product in attention score is to ensure that the dot-products between query and key do not grow too large for large $d_k$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3c9bf8-9c28-4519-8d41-8b61eb4b66ec",
   "metadata": {},
   "source": [
    "# Multi-Head Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa64203-155a-487e-9ce7-c1f484248cd3",
   "metadata": {},
   "source": [
    "This is the last step to understand the Transformer model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e3b03b-54bd-48ed-9154-cbb50dc5ba15",
   "metadata": {},
   "source": [
    "![](../images/mha_img_original.png \"Multi-Head Attention\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3f5969-c90a-45a7-a01d-78b158ee6125",
   "metadata": {},
   "source": [
    "If we recall that \n",
    "\n",
    "$$Attention(q,k,v) = softmax(\\frac{qk^T}{\\sqrt{d_k}})v\\newline$$\n",
    ", it computes only one head. In order to turn that into a Multi-Head Attention, surprise, surprise, we stack h number of such heads with different $W^q, W^k, W^v$ and concatenate them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006ba8cf-7b27-498c-9dbb-a0efd15323e3",
   "metadata": {},
   "source": [
    "The reason why the model has multiple heads is that one head focus only on one aspect of similarity. Thus, multiple heads allows the model to focus on several aspects at once.\n",
    "Finally, here is the Transformer architecture:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2fef0c2-429e-47e7-b04c-f84e4b1a982a",
   "metadata": {},
   "source": [
    "![](../images/transformer_arch.png \"Transformer Architecure\")\n",
    "\n",
    "As shown in the figure above, the Transformer model consists of Multi-Head Attention and MLP (Feed Forward) blocks. In addition, Layer Norm is used before every block and residual connections like in ResNet after every block."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abfe9202-04dc-402b-81aa-366e08c105e9",
   "metadata": {},
   "source": [
    "# ViT - AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bffb454b-03c2-485e-b893-baba3a90838a",
   "metadata": {},
   "source": [
    "![](../images/ViT.png \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d03bce0-7e93-4421-99de-4d06dd95b51f",
   "metadata": {},
   "source": [
    "Architecture above is Vision Transformer model architecrue from the original paper. As it is seen, we split the input image into a fixed sized pieces(patches). It is because of the Transformer architecture. It accepts input as a sequence. The overall steps how the model works are as follows:\n",
    "1. Split an input image into patches(pieces).\n",
    "2. Linearly embed each of the patches.\n",
    "3. Add position embeddings\n",
    "4. Feed the resulting sequence of vectors to standard Transformer Encoder and get the output for each of [cls] tokens.\n",
    "5. Pass the representation of [cls] tokens through an MLP Head to get final class predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e492bb69-8d0d-4cfe-8d39-fe3cf6c55ec6",
   "metadata": {},
   "source": [
    "## Patch Embedings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afc213d-5d68-49b8-ba64-acc85e643d3f",
   "metadata": {},
   "source": [
    "Now, we will take a closer look at each one of the steps with code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154a16d5-f062-4ce7-a67a-3e670b1bdd8f",
   "metadata": {},
   "source": [
    "> Note: All PyTorch code is taken from [here](https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqbkU0T28yU3dsbDJMREsyX01BMGJQZGtuLW5oUXxBQ3Jtc0tseERTOWlwczRfWHlVRUxkUWRtRVJvVTkxU1A4YmwwcTlPaWR2YVo3N2xzbzdZRHYyWVprU01ZRkRMSzRRMUwyMVdnclBXckt2UE13VndURGlxNVB5NUJYNWJ1VjRuNjMxUmRUaEdmWC04V3R6SGpjVQ&q=https%3A%2F%2Fgithub.com%2Fjankrepl%2Fmildlyoverfitted%2Ftree%2Fmaster%2Fgithub_adventures%2Fvision_transformer&v=ovB0ddFtzzA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c65c0af-6978-43d8-9a95-ab6582481a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#collapse\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d2afb33-d020-4630-9b10-47245f2cbcbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#collapse\n",
    "class PatchEmbed(nn.Module):\n",
    "    \"\"\"\n",
    "    Split image into patches and embed them.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    img_size: Size of the image\n",
    "    patch_size: Size of the path\n",
    "    in_chans: Number of input channels\n",
    "    embed_dim: The embedding dimension\n",
    "        \n",
    "    Attributes\n",
    "    ----------\n",
    "    n_patches: int\n",
    "        Number of patches inside of our image.\n",
    "    proj: nn.Conv2D\n",
    "        Convolutional layer that does both the splitting into patches and teir embedding.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size: int, path_size: int, in_chans: int = 3, embed_dim: int = 768):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.n_patches = (img_size // patch_size) ** 2\n",
    "        \n",
    "        self.proj = nn.Conv2d(\n",
    "            in_chans,\n",
    "            embed_dim,\n",
    "            kernel_size=patch_size,\n",
    "            stride=patch_size,\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\" Forward pass\n",
    "        Parameters\n",
    "        ----------\n",
    "        x: Shape\n",
    "        \"\"\"\n",
    "        x = self.proj(\n",
    "            x\n",
    "        )\n",
    "        x = x.flatten(2)\n",
    "        x = x.transpose(1, 2)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6527301-053a-45a2-9801-3b8a19ac9ecf",
   "metadata": {},
   "source": [
    "We will start with the 1st and 2nd step combined. Insted of dividing an input image into patches and then doing Linear Projection step, we combine them with the help of Convolution. The module accepts image size, patch size (originally 16x16), channels (1 - if it is graysclae, 3 - if it is RGB) and embedding dimension (stays constant across the entire network). Pay attention to Convolution operation - we set the kernel_size and stride equal to path_size. This way, the kernel exactly fall into the patches without overlapping. This way we get Patch Embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051cd01b-6cb8-4076-b76e-1bb6a67b578e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40f31c59-0001-42fd-a9da-8bcf689928f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#collapse\n",
    "class Attention(nn.Module):\n",
    "    \"\"\"Attention mechanism.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dim: The input and output dimension of per token features.\n",
    "    \n",
    "    n_heads: Number of attention heads\n",
    "    \n",
    "    qkv_bias: If True then we include bias to the query, key and value projections.\n",
    "    \n",
    "    attn_p: Dropout probability applied to the query, key and value tensors.\n",
    "    \n",
    "    proj_p: Dropout probability applied to the output tensor\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    scale: Normalizing for the dot product\n",
    "    \n",
    "    qkv: Linear projection for the query, key and value.\n",
    "    \n",
    "    proj: Linear mapping that takes in the concatenated output of all attention heads and maps it into a new space.\n",
    "    \n",
    "    attn_drop, proj_drop: Droput layers.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dim: int, n_heads: int = 12, qkv_bias: bool = True, attn_p: float = 0.2, proj_p: float = 0.2):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.dim = dim\n",
    "        self.head_dim = dim // n_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "        \n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_p)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Run forward pass.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x : Shape (n_samples, n_patches + 1, dim).\n",
    "        \"\"\"\n",
    "        n_samples, n_tokens, dim = x.shape\n",
    "        \n",
    "        qkv = self.qkv(x)\n",
    "        qkv = qkv.reshape(\n",
    "            n_samples, n_tokens, 3, self.n_heads, self.head_dim\n",
    "        )\n",
    "        qkv = qkv.permute(\n",
    "            2, 0, 3, 1, 4\n",
    "        )\n",
    "            \n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        k_t = k.transpose(-2, -1)\n",
    "        dp = (\n",
    "           q @ k_t\n",
    "        ) * self.scale \n",
    "        attn = dp.softmax(dim=-1) \n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        weighted_avg = attn @ v  \n",
    "        weighted_avg = weighted_avg.transpose(\n",
    "                1, 2\n",
    "        )\n",
    "        weighted_avg = weighted_avg.flatten(2) \n",
    "\n",
    "        x = self.proj(weighted_avg) \n",
    "        x = self.proj_drop(x)  \n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "facd4c58-8fd4-4d1c-8535-bd0baa12d8a0",
   "metadata": {},
   "source": [
    "The above code does 4 things:\n",
    "1. Project each patch embedding into three vectors called query, key and value.\n",
    "2. Compute attention scores using dot-product. Queries and keys that are similar will have a large dot product.\n",
    "3. Compute attention weights. Dot products can generate large numbers, which can destabilize the training process. To handle this, the attention scores are first multiplied by a scaling vector and then normalized with a softmax to ensure all the column values add up to 1.\n",
    "4. Update the embeddings by multiplying them with the value vector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d2eae0-6b95-4c63-a4c3-fbf3add94c11",
   "metadata": {},
   "source": [
    "## MLP Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e082657-76fe-46ef-853c-cc91550a8692",
   "metadata": {},
   "outputs": [],
   "source": [
    "#collapse\n",
    "class MLP(nn.Module):\n",
    "    \"\"\"Multilayer perceptron.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    in_features : Number of input features.\n",
    "    hidden_features : Number of nodes in the hidden layer.\n",
    "    out_features : Number of output features.\n",
    "    p : Dropout probability.\n",
    "    Attributes\n",
    "    ----------\n",
    "    fc : nn.Linear\n",
    "        The First linear layer.\n",
    "    act : nn.GELU\n",
    "        GELU activation function.\n",
    "    fc2 : nn.Linear\n",
    "        The second linear layer.\n",
    "    drop : nn.Dropout\n",
    "        Dropout layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features: int, hidden_features: int, out_features: int, p: float = 0.2):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = nn.GELU()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(p)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Run forward pass.\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : Shape\n",
    "        Returns\n",
    "        -------\n",
    "        Shape\n",
    "        \"\"\"\n",
    "        x = self.fc1(\n",
    "                x\n",
    "        ) \n",
    "        x = self.act(x) \n",
    "        x = self.drop(x)  \n",
    "        x = self.fc2(x)  \n",
    "        x = self.drop(x)  \n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0e35b4-ae28-495d-b2c5-3f2f7596025d",
   "metadata": {},
   "source": [
    "There is nothing extraordinary in the above code except GELU activation function. This is classification part consisting of just two fully connected layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a536cf29-e97b-466d-a39e-e01d5b749b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "#collapse\n",
    "class Block(nn.Module):\n",
    "    \"\"\"Transformer block.\n",
    "    Parameters\n",
    "    ----------\n",
    "    dim : Embeddinig dimension.\n",
    "    n_heads : Number of attention heads.\n",
    "    mlp_ratio : Determines the hidden dimension size of the `MLP` module with respect\n",
    "        to `dim`.\n",
    "    qkv_bias : If True then we include bias to the query, key and value projections.\n",
    "    p, attn_p : Dropout probability.\n",
    "    Attributes\n",
    "    ----------\n",
    "    norm1, norm2 : LayerNorm\n",
    "        Layer normalization.\n",
    "    attn : Attention\n",
    "        Attention module.\n",
    "    mlp : MLP\n",
    "        MLP module.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim: int, n_heads: int, mlp_ratio:float = 4.0, qkv_bias: bool = True, p: float = 0., attn_p: float = 0.):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(dim, eps=1e-6)\n",
    "        self.attn = Attention(\n",
    "                dim,\n",
    "                n_heads=n_heads,\n",
    "                qkv_bias=qkv_bias,\n",
    "                attn_p=attn_p,\n",
    "                proj_p=p\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(dim, eps=1e-6)\n",
    "        hidden_features = int(dim * mlp_ratio)\n",
    "        self.mlp = MLP(\n",
    "                in_features=dim,\n",
    "                hidden_features=hidden_features,\n",
    "                out_features=dim,\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Run forward pass.\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : Shape.\n",
    "        \n",
    "        \"\"\"\n",
    "        x = x + self.attn(self.norm1(x))\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ccaf8c1-2398-4205-8802-d91fd48df611",
   "metadata": {},
   "source": [
    "In ViT authors used only one block (Encoder block). Original paper uses post layer normalization arrangement when normalization layer is placed in between the skip connections. However, it is hard to train from scratch and therefore the code above uses pre layer normalization arrangement.\n",
    "![](../images/norm_types.jpg \"Normalization types\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c0a143-25ac-431a-a7a0-342379046192",
   "metadata": {},
   "source": [
    "## Vision Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d3ffe3dc-b86a-4fa2-ae02-16a3388002c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#collapse\n",
    "class VisionTransformer(nn.Module):\n",
    "    \"\"\"Simplified implementation of the Vision transformer.\n",
    "    Parameters\n",
    "    ----------\n",
    "    img_size : Both height and the width of the image (it is a square).\n",
    "    patch_size : Both height and the width of the patch (it is a square).\n",
    "    in_chans : Number of input channels.\n",
    "    n_classes : Number of classes.\n",
    "    embed_dim : Dimensionality of the token/patch embeddings.\n",
    "    depth : Number of blocks.\n",
    "    n_heads : Number of attention heads.\n",
    "    mlp_ratio : Determines the hidden dimension of the `MLP` module.\n",
    "    qkv_bias : If True then we include bias to the query, key and value projections.\n",
    "    p, attn_p : Dropout probability.\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    patch_embed : PatchEmbed\n",
    "        Instance of `PatchEmbed` layer.\n",
    "    cls_token : nn.Parameter\n",
    "        Learnable parameter that will represent the first token in the sequence.\n",
    "        It has `embed_dim` elements.\n",
    "    pos_emb : nn.Parameter\n",
    "        Positional embedding of the cls token + all the patches.\n",
    "        It has `(n_patches + 1) * embed_dim` elements.\n",
    "    pos_drop : nn.Dropout\n",
    "        Dropout layer.\n",
    "    blocks : nn.ModuleList\n",
    "        List of `Block` modules.\n",
    "    norm : nn.LayerNorm\n",
    "        Layer normalization.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            img_size: int = 384,\n",
    "            patch_size: int = 16,\n",
    "            in_chans: int = 3,\n",
    "            n_classes: int = 1000,\n",
    "            embed_dim: int = 768,\n",
    "            depth: int = 12,\n",
    "            n_heads: int = 12,\n",
    "            mlp_ratio: int = 4.,\n",
    "            qkv_bias: int = True,\n",
    "            p: int = 0.2,\n",
    "            attn_p: int = 0.2,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.patch_embed = PatchEmbed(\n",
    "                img_size=img_size,\n",
    "                patch_size=patch_size,\n",
    "                in_chans=in_chans,\n",
    "                embed_dim=embed_dim,\n",
    "        )\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.pos_embed = nn.Parameter(\n",
    "                torch.zeros(1, 1 + self.patch_embed.n_patches, embed_dim)\n",
    "        )\n",
    "        self.pos_drop = nn.Dropout(p=p)\n",
    "\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [\n",
    "                Block(\n",
    "                    dim=embed_dim,\n",
    "                    n_heads=n_heads,\n",
    "                    mlp_ratio=mlp_ratio,\n",
    "                    qkv_bias=qkv_bias,\n",
    "                    p=p,\n",
    "                    attn_p=attn_p,\n",
    "                )\n",
    "                for _ in range(depth)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.norm = nn.LayerNorm(embed_dim, eps=1e-6)\n",
    "        self.head = nn.Linear(embed_dim, n_classes)\n",
    "\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Run the forward pass.\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : Shape `(n_samples, in_chans, img_size, img_size)`.\n",
    "        Returns\n",
    "        -------\n",
    "        logits : Logits over all the classes - `(n_samples, n_classes)`.\n",
    "        \"\"\"\n",
    "        n_samples = x.shape[0]\n",
    "        x = self.patch_embed(x)\n",
    "\n",
    "        cls_token = self.cls_token.expand(\n",
    "                n_samples, -1, -1\n",
    "        ) \n",
    "        x = torch.cat((cls_token, x), dim=1)  \n",
    "        x = x + self.pos_embed  \n",
    "        x = self.pos_drop(x)\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        x = self.norm(x)\n",
    "\n",
    "        cls_token_final = x[:, 0]  # just the CLS token\n",
    "        x = self.head(cls_token_final)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec95227-a0a5-4c19-b98b-c3c3f56e1b93",
   "metadata": {},
   "source": [
    "Finally, it is time to assemble all the code blocks into one module to create Vision Transformer model. The code incorporates positional embedding and a `[cls]` token into the model and then add all other modules."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a34f8fe-1635-4847-8849-c9ed92f64a68",
   "metadata": {},
   "source": [
    "In this blog post we covered all about Transformer architecture, starting from encoder-decoder framework to application of Transformer architecture in Computer Vision. In the next post, we will apply the model in a classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec6375a-e2d4-418d-a60c-62b83116269d",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35338d78-39cc-49aa-8f8d-c13b77173e70",
   "metadata": {},
   "source": [
    "{% bibliography --cited %}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d95070d-4f8c-4982-8cfa-0668fd3dab4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
